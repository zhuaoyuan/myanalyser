# myanalyser 系统设计

## 1. 目标与范围

本项目面向自用基金分析场景，覆盖数据采集、清洗校验、回测与榜单生成。  
本文件描述目录分层、核心模块职责、数据流和统一验收机制，用于保证工程化可维护。

## 2. 目录分层

```text
myanalyser/
  src/        # 业务代码与 CLI
    contracts/   # 中间产物契约定义
    validators/  # 契约校验器
    transforms/  # 流程复用转换脚本
  tests/      # 单元测试与集成测试
  data/       # 原始/中间/样例数据
  artifacts/  # 运行产物
  docs/       # 文档
  tools/      # 一次性脚本
```

设计原则：

- 业务代码与测试分离（`src/` 与 `tests/`）
- 数据与代码分离（`data/` 与 `src/`）
- 文档与实现同步维护（`docs/`）

## 3. 数据目录与版本化

统一约定：

- 公共数据：`data/common/`（如 `trade_dates.csv`、`fund_blacklist.csv`）
- 样例数据：`data/samples/`
- 跑数版本数据：`data/versions/{run_id}/`
- 每个版本的 ETL 结果：`data/versions/{run_id}/fund_etl/`
- 每个版本的错误日志：`data/versions/{run_id}/logs/`

`run_id` 规则：

- 默认：`YYYYMMDD_HHMMSS`
- 可拼接描述后缀：`YYYYMMDD_HHMMSS_desc`

该结构确保每次测试/正式跑数数据彼此隔离、可回溯、可复现。

## 4. 核心模块职责

- `src/fund_etl.py`：AkShare 数据采集（step1~step7、retry）
- `src/adjusted_nav_tool.py`：复权净值计算
- `src/compare_adjusted_nav_and_cum_return.py`：复权收益率与累计收益率一致性比对
- `src/check_trade_day_data_integrity.py`：交易日数据完整性检查
- `src/filter_funds_for_next_step.py`：基于 Step 9 与交易日完整性结果过滤基金清单
- `src/pipeline_scoreboard.py`：评分榜单计算与导出（支持 `--formal-only`、`--skip-sinks`、`--latest-nav-date`、`--clickhouse-write-profile`、`--clickhouse-write-scope`）
- `src/scoreboard_metrics.py`：评分榜指标计算共享模块（供 pipeline 与 verify_scoreboard_recalc 共用）
- `src/backtest_portfolio.py`：回测流程执行
- `src/contracts/pipeline_contracts.py`：关键中间产物契约定义（列名/类型/非空/唯一键 + 目录 CSV 文件数量）
- `src/validators/validate_pipeline_artifacts.py`：按 stage 执行契约校验（失败即非 0）
- `src/transforms/build_effective_purchase_csv.py`：从 fund_purchase 剔除黑名单生成 fund_purchase_effective.csv（不修改原始 purchase）
- `src/transforms/build_filtered_purchase_csv.py`：过滤结果到 step10 输入清单的统一转换脚本

关键输入输出边界：

- 输入优先从 `data/versions/{run_id}/fund_etl/` 读取
- 日志优先写入 `data/versions/{run_id}/logs/`
- 可视化/报告类产物优先写入 `artifacts/`

契约校验策略：

- 校验逻辑放在具体 Python 环节入口（`fund_etl`、`adjusted_nav_tool`、`check_trade_day_data_integrity`、`compare_adjusted_nav_and_cum_return`、`filter_funds_for_next_step`、`pipeline_scoreboard`、`verify_scoreboard_recalc`），避免只在 shell 脚本重复校验导致分叉。
- `*_by_code` 目录不逐文件读取，仅校验目录存在且 CSV 文件数量达到下限。

基金黑名单机制：

- 黑名单文件：`data/common/fund_blacklist.csv`（含 `基金代码` 列），可通过 `FUND_BLACKLIST_PATH` 覆盖。
- `fund_purchase.csv` 为原始申购清单，**不修改**；`fund_purchase_effective.csv` = fund_purchase − 黑名单。
- 流程中 step2~step7、filter、build_filtered_purchase 均依赖 `fund_purchase_effective.csv`。
- 运行报告记录：原始基金数、黑名单剔除数、有效基金数。

评分榜历史截断口径：

- 当设置 `pipeline_scoreboard.py --latest-nav-date=YYYY-MM-DD` 时，指标计算仅使用 `净值日期 <= latest_nav_date` 的样本。
- 导出字段中的 `期末日期`、周期指标窗口终点、以及 `最近人事变动日期` 都按同一截断日计算。
- 为避免 checkpoint 口径错配，设置 `--latest-nav-date` 时会自动禁用 `--resume`。
- 对同一份历史截断 scoreboard 做核验时，`verify_scoreboard_recalc.py` 必须传入同值 `--latest-nav-date`。

## 5. 验收跑机制（E2E）

统一脚本：`tools/verify.sh`

执行顺序（11 步，含 9.5 过滤）：

1. 单测回归（`python -m unittest discover`）
2. 核心 CLI smoke（5 个核心 CLI 的集成测试）
3. 启动 `fund_db_infra` 并检查 MySQL/ClickHouse 健康状态
4. 执行 `fund_etl verify + step1`
5. 将 `fund_purchase.csv` 抽样为 21 只（前 20 + `163402`）
5.5. 执行 `build_effective_purchase_csv`，生成 `fund_purchase_effective.csv`
6. 执行 `fund_etl step2~step7`（使用 `fund_purchase_effective.csv`）
7. 执行复权净值计算（`adjusted_nav_tool.py`）
8. 执行交易日完整性检查（`check_trade_day_data_integrity.py`）
9. 执行复权收益率一致性比对（`compare_adjusted_nav_and_cum_return.py`）
9.5. 执行基金过滤（`filter_funds_for_next_step.py`），并生成 `fund_purchase_for_step10_filtered.csv`
10. 执行榜单入库与导出 + 2025 年回测（消费过滤后 purchase，`pipeline_scoreboard.py` 完整流程含 DB + `backtest_portfolio.py`）
   - 验收默认使用 `--clickhouse-write-profile fast`
   - 验收默认使用 `--clickhouse-write-scope verify_minimal`（仅写 `fact_fund_nav_daily` 与 `fact_fund_scoreboard_snapshot`）
11. 执行评分榜独立重算核验（`verify_scoreboard_recalc.py`），要求 `summary.csv` 全部通过

验收通过标准：

- 单测全绿
- 核心 CLI smoke 全绿
- ETL/校验/回测链路均有产物且关键 CSV 非空
- DB Sink 写入无阻断（MySQL + ClickHouse）
- 评分榜重算核验（`scoreboard_recheck/summary.csv`）全部通过
- 运行报告可读：`run_report_summary.csv` 与 `run_report.md` 可直接给出步骤耗时/成功率/异常分布/过滤前后数量变化
  - Step10 额外打印 `scoreboard_seconds` / `backtest_seconds`，快速区分榜单链路与回测链路耗时

## 6. 正式跑机制（Full Run）

统一脚本：`tools/run_full_pipeline.sh`
推荐启动器：`tools/start_isolated_pipeline.sh`（长时任务）

定位：

- 面向正式生产跑数，不包含单测/CLI smoke，不做验收抽样（验收跑为 21 只）
- 执行全量 ETL（`fund_etl --mode all`）并保留 Step 9.5 过滤
- 正式跑使用 `--formal-only`（纯 Python 计算，不写 DB），在评分 CSV 导出处结束；回测链路由后续独立流程执行
- 若用于历史回测，应显式指定 `--latest-nav-date`，确保评分只基于该时点之前信息
- 支持同 `run_id` 断点续跑：步骤成功后写 checkpoint，重跑优先复用已完成结果
- 长时任务通过 `start_isolated_pipeline.sh` 在独立 `git worktree` 中运行，避免开发目录代码变更影响正在执行的正式任务

执行顺序（9 步）：

1. 启动 `fund_db_infra` 并检查 MySQL/ClickHouse 健康状态
1a. 准备 `fund_purchase.csv`（本地复制或 fund_etl step1）
1b. 执行 `build_effective_purchase_csv`，生成 `fund_purchase_effective.csv`
2. 执行 `fund_etl verify + step2~step7`（使用 `fund_purchase_effective.csv`）
3. 执行复权净值计算（`adjusted_nav_tool.py`）
4. 执行交易日完整性检查（`check_trade_day_data_integrity.py`）
5. 执行复权收益率一致性比对（`compare_adjusted_nav_and_cum_return.py`）
6. 执行基金过滤（`filter_funds_for_next_step.py`），生成过滤结果与 `fund_purchase_for_step10_filtered.csv`
7. 执行榜单入库与导出（消费过滤后 purchase，`pipeline_scoreboard.py`）

隔离启动流程（推荐）：

1. 读取当前 `HEAD` 并创建 detached worktree（默认目录：`../finance-runs/run_{timestamp}`）
2. 记录运行元数据（`VERSION_INFO`、`LAUNCH_INFO`）
3. 以 `nohup` 后台执行 `tools/run_full_pipeline.sh`，日志写入 `pipeline.log`
4. 若传入 `--venv`，为子进程注入 `VIRTUAL_ENV` 与 `PATH`，确保运行环境固定

环境变量注入策略：

- `run_full_pipeline.sh` 可通过命令前缀环境变量覆盖默认值（如 `ETL_MAX_WORKERS`、`FILTER_START_DATE`、`DATA_VERSION`）。
- 使用 `start_isolated_pipeline.sh` 启动时，上述环境变量会透传至后台任务。

正式跑关键产物：

- `data/versions/{run_id}/fund_etl`
- `data/versions/{run_id}/logs`
- `artifacts/full_run_{run_id}/filtered_fund_candidates.csv`
- `artifacts/full_run_{run_id}/scoreboard`
- `artifacts/full_run_{run_id}/.checkpoints`（步骤完成标记）
- `artifacts/full_run_{run_id}/run_report_steps.csv`
- `artifacts/full_run_{run_id}/run_report_summary.csv`
- `artifacts/full_run_{run_id}/run_report.md`

## 9. 最小回归基线设计

- 目录：`tests/baseline/mini_case/`
- 数据来源：历史 `data/versions` 与 `artifacts` 的真实产物抽样（固定样本 + 固定期望输出）。
- 用例：`tests/test_pipeline_regression_baseline.py`
  - `filter_funds_for_next_step`
  - `build_filtered_purchase_csv`
  - `pipeline_scoreboard --formal-only`
  - `verify_scoreboard_recalc`
- 期望目录可参数化：通过 `MYANALYSER_BASELINE_EXPECTED_DIR` 指定，默认 `tests/baseline/mini_case/expected/default`。

## 7. 数据库与写入稳定性设计

### 7.1 业务表说明

MySQL（维度与过滤结果）：

- `dim_fund_base`：基金基础维表。沉淀基金静态属性、费率、开放状态与更新时间，作为评分榜单宽表的维度来源。
- `dim_fund_exclusion_detail`：基金过滤明细表。记录每只基金在 Step 9.5 过滤阶段被剔除的具体原因（可一只基金多原因）。
- `dim_fund_exclusion_summary`：基金过滤汇总表。按剔除原因聚合基金数量，用于快速查看过滤规则影响面。

ClickHouse（事实与分析快照）：

- `fact_fund_nav_daily`：基金日净值事实表。存储单位净值、复权净值与日收益率，是区间收益与指标计算的基础明细层。
- `fact_fund_return_period`：基金周期收益事实表。按周/月/季聚合区间收益率，为胜率与波动率计算提供输入。
- `fact_fund_metrics_snapshot`：基金指标快照表。存储单基金在全样本、近 3 年、近 1 年窗口的收益/波动/回撤/比率指标。
- `fact_fund_scoreboard_snapshot`：基金评分榜单快照表。存储最终榜单所需的指标值、排名结果及维度字段，用于导出与下游展示。

- MySQL `data_version` 字段采用 `VARCHAR(64)`，避免版本号长度被 `CHAR(6)` 限制。
- ClickHouse 写入采取“分区感知 + 小批次聚合写入”：
  - 按分区键预分组，避免单次 INSERT 覆盖超过 `max_partitions_per_insert_block`。
  - 将分组按批打包，降低碎片化写入与 `TOO_MANY_PARTS` 风险。
  - 对大表设置 `chunk_rows`，控制单批 CSV 体积，降低内存尖峰。
- 写入策略支持 profile/scope 两维控制：
  - `clickhouse-write-profile`: `safe`（保守限速）/`fast`（低限速 + 并行插入）/`auto`（按数据量自动切换）
  - `clickhouse-write-scope`: `full`（全部事实表）/`verify_minimal`（仅回测必需表）
- 问题排查优先看：
  - `system.merges`（后台合并压力）
  - `system.error_log`（`MEMORY_LIMIT_EXCEEDED`/`TOO_MANY_PARTS`）

## 8. 兼容性与演进策略

- 不保留根目录兼容入口，统一 `python src/xxx.py`
- 通过文档和统一验收脚本约束路径改动，降低后续回归风险
- 若引入 `make verify`，建议仅转调 `bash tools/verify.sh`，避免逻辑分叉
