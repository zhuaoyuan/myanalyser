# 需求日志：端到端集成验收与数据库入库优化

## 1. 背景

本次需求目标是将 `tools/verify.sh` 从“单测 + CLI smoke”升级为“覆盖数据库的端到端集成验收”，范围包含：

- 数据抓取（`fund_etl step1~step7`）
- 复权计算（`adjusted_nav_tool.py`）
- 数据验证（交易日完整性 + 本地远程收益一致性）
- 数据入库（MySQL + ClickHouse）
- 分析与回测报告（`pipeline_scoreboard.py` + `backtest_portfolio.py`）

并要求使用独立测试版本号管理本地目录和数据库写入版本。

## 2. 关键对话与决策

### 2.1 验收脚本升级策略

- 开发者要求：`verify.sh` 增加完整 E2E 流程，抽样规则固定为“前 100 + 163402（共 101）”。
- AI 实现：将脚本扩展为 10 步流程，并对每步增加“有文件/有数据”的基础断言。

### 2.2 关于降载策略的讨论

- AI 初始为排障加入了临时降载（`code-limit`）以先验证链路可达。
- 开发者明确不接受以降载替代稳定性优化，要求从根因入手。
- 最终决策：保留全量测试诉求，转为优化数据库 schema 与 ClickHouse 写入策略。

### 2.3 数据库相关决策

- MySQL：`data_version` 从 `CHAR(6)` 扩展为 `VARCHAR(64)`，并增加兼容 `ALTER TABLE`。
- ClickHouse：写入逻辑从“高碎片小分区多次写入”调整为“分区感知 + 分组批量写入 + 行数分块”，降低：
  - `TOO_MANY_PARTS`
  - `max_partitions_per_insert_block` 触发概率
  - 背景 merge 内存压力

## 3. 实现摘要

### 3.1 脚本与代码变更

- `myanalyser/tools/verify.sh`
  - 升级为 10 步端到端验收
  - 启动并健康检查 `fund_db_infra`（MySQL/ClickHouse）
  - 执行 ETL 抽样 + 预处理 + 验证 + 入库 + 回测
  - 支持 `RUN_ID` 与 `DATA_VERSION`
  - 增加 `python/python3` 兼容探测

- `fund_db_infra/sql/mysql_schema.sql`
  - `data_version` 字段扩容到 `VARCHAR(64)`
  - 增加已有表兼容迁移 `ALTER TABLE ... MODIFY COLUMN`

- `myanalyser/src/pipeline_scoreboard.py`
  - 优化 ClickHouse 批量写入：
    - 分区分组
    - 分组打包批次写入
    - `chunk_rows` 分块
  - 降低单次 INSERT 跨分区过多导致的失败风险

### 3.2 元文件同步

- `myanalyser/docs/README.md`
- `myanalyser/docs/系统设计.md`
- `myanalyser/docs/项目通用约束.md`
- 本日志文件

## 4. 异常与降级记录

### 4.1 异常一：MySQL 入库失败

- 现象：`Data too long for column 'data_version'`
- 受影响环节：`pipeline_scoreboard.py` 写入 MySQL
- 根因：schema 中 `data_version CHAR(6)` 与验收版本号长度不匹配
- 处理：扩容 schema 并兼容迁移

### 4.2 异常二：ClickHouse 入库失败

- 现象：写入阶段出现 `TOO_MANY_PARTS` / `MEMORY_LIMIT_EXCEEDED`
- 受影响环节：`fact_fund_nav_daily` / `fact_fund_return_period` 批量写入
- 根因：单次 INSERT 分区覆盖与写入粒度不合理，触发大量 part 与后台 merge 压力
- 处理：改造为分区感知批量写入并控制 chunk

## 5. 验收证据

执行命令（端到端）：

```bash
cd /Users/zhuaoyuan/cursor-workspace/finance
source myanalyser/.venv312/bin/activate
cd myanalyser
RUN_ID=20260226_full_e2e_verify DATA_VERSION=20260226_full_e2e_db_opt3 bash tools/verify.sh
```

关键结果（示例）：

- ETL 抽样链路完成（101 只抽样，含 `163402`）
- 复权净值产出成功（`fund_adjusted_nav_by_code` 非空）
- 交易日完整性报告产出：
  - `artifacts/verify_<run_id>/trade_day_integrity_reports/trade_day_integrity_summary_2025-01-01_2025-12-31.csv`
- 收益率一致性比对报告产出：
  - `artifacts/verify_<run_id>/fund_return_compare/summary.csv`
- 榜单导出成功（示例：`scoreboard_rows=94`）：
  - `artifacts/verify_<run_id>/scoreboard/fund_scoreboard_<data_version>.csv`
- 回测报告产出：
  - `artifacts/verify_<run_id>/backtest/backtest_report.md`

## 6. 后续建议

- 将 `fund_db_infra` 的资源配额（内存/磁盘）参数化，便于不同机器复现。
- 在 `pipeline_scoreboard.py` 中增加写入重试与失败重放机制（按分区键重试）。
- 为 ClickHouse 写入过程增加结构化诊断输出（记录批次大小、分区数、耗时）。
